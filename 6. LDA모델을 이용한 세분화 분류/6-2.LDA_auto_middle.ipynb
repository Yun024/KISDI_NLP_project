{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ab3e6-480b-40fa-98df-b3daa006d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://bab2min.github.io/tomotopy/v0.12.3/kr/#tomotopy.LDAModel\n",
    "# Package Load\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tomotopy as tp\n",
    "import tomotopy.coherence as tpc\n",
    "from gensim.corpora import Dictionary\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from timeit import default_timer\n",
    "import plotly.express as px\n",
    "import plotly.offline\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import operator\n",
    "import scipy\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d5e57-7bf0-47a4-833b-1edc538d0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_topic_num = int(input(\"LDA 모델 최적 토픽 수를 검증하기 위한 최대 토픽 수를 입력하세요. (※ 10이상의 자연수, 권장값 = 250개)\"))\n",
    "iteration = int(input(\"학습 Iteration 수를 입력하세요. (※ 100이상의 자연수, 권장값 = 1000회)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa9033-b265-4574-b12d-038a25df8e43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \".\\\\data\\\\LDA\\\\NTIS\\\\corpus_topic\\\\\"\n",
    "\n",
    "for topic_ver in tqdm(range(len(os.listdir(DATA_DIR)))):\n",
    "    \n",
    "    # 코퍼스 다운로드\n",
    "    corpus = []\n",
    "    for n, line in enumerate(open(DATA_DIR + \"LDA_topic_\"+str(topic_ver+1)+\".txt\", encoding='CP949')):\n",
    "        doc=line.strip().split()\n",
    "        corpus.append(doc)\n",
    "    \n",
    "    # min_cf 자동 입력\n",
    "    sct = sum(corpus,[])\n",
    "    df_sct = pd.DataFrame(Counter(sct).most_common(),columns=[\"word\",\"count\"])\n",
    "    mcf = df_sct.iloc[:int(len(df_sct)*  0.2),:][\"count\"].iloc[-1]-1\n",
    "    \n",
    "    # LDA모델 최적화\n",
    "   \n",
    "    score_c_list=[]\n",
    "   \n",
    "    for opt_tp in range(5, upper_topic_num+5, 5): \n",
    "        model=tp.LDAModel(k=opt_tp, min_cf=mcf, rm_top=64 ,tw=tp.TermWeight.PMI, seed=42)\n",
    "        for cnt in range(len(corpus)):\n",
    "            model.add_doc(corpus[cnt])\n",
    "        model.train(iter=iteration, workers=0)\n",
    "        coherence_c=tpc.Coherence(model, coherence='c_v')\n",
    "        score_c=coherence_c.get_score()\n",
    "        score_c_list.append(score_c)\n",
    "    \n",
    "    max_index = score_c_list.index(max(score_c_list)) +1 \n",
    "    \n",
    "    score_c_list_2=[]\n",
    "\n",
    "    for opt_tp in range(max_index*5-5, max_index*5+5, 1): \n",
    "        model=tp.LDAModel(k=opt_tp, min_cf=mcf, rm_top=64 ,tw=tp.TermWeight.PMI, seed=42)\n",
    "        for cnt in range(len(corpus)):\n",
    "            model.add_doc(corpus[cnt])\n",
    "        model.train(iter=iteration, workers=0)\n",
    "        coherence_c_2=tpc.Coherence(model, coherence='c_v')\n",
    "        score_c_2=coherence_c_2.get_score()\n",
    "        score_c_list_2.append(score_c_2)\n",
    "        \n",
    "    score_c_list_2.index(max(score_c_list_2))\n",
    "    \n",
    "    \n",
    "    # 토픽개수 결정\n",
    "    topic_num = range(max_index*5-5, max_index*5+5, 1)[score_c_list_2.index(max(score_c_list_2))]\n",
    "\n",
    "\n",
    "    # LDA 모델 학습 \n",
    "\n",
    "    s = default_timer()\n",
    "\n",
    "    mdl=tp.LDAModel(min_cf=mcf, rm_top=64, k=topic_num, alpha=0.1, eta=0.01, tw=tp.TermWeight.PMI, seed=42)\n",
    "    for cnt in range(len(corpus)):\n",
    "        mdl.add_doc(corpus[cnt])\n",
    "    mdl.train(iter=iteration, workers=0)\n",
    "\n",
    "    e = default_timer()\n",
    "    mdl.save('./model/middle_model/LDA_MODEL_topic_'+str(topic_ver+1)+'.bin')\n",
    "    \n",
    "    # 각 토픽 별 시트 저장 \n",
    "    final = pd.DataFrame()\n",
    "    for j in range(mdl.k):\n",
    "        word = [\"Topic_\"+ str(j+1)]\n",
    "        topic_word = pd.DataFrame(np.array(mdl.get_topic_words(j,top_n=15))[:,0].tolist(),columns=word)\n",
    "        final = pd.concat([final,topic_word],axis=1)\n",
    "    \n",
    "    if topic_ver==0:\n",
    "        final.to_excel(excel_writer=\"./data\\\\LDA\\\\NTIS\\\\topic_middle_word.xlsx\",\n",
    "                      sheet_name=\"topic1\",index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        file_name = \"./data\\\\LDA\\\\NTIS\\\\topic_middle_word.xlsx\"\n",
    "        writer = pd.ExcelWriter(file_name,mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"overlay\")\n",
    "        final.to_excel(writer,sheet_name=\"topic\"+str(topic_ver+1),startcol=0,startrow=0,index=False)\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "        \n",
    "        \n",
    "    path = \"C:\\\\Users\\\\KISDI\\\\LDA\\\\html\\\\LDA\\\\NTIS\\\\Topic_Network_NTIS_middle\\\\\"+\"Topic_\"+str(topic_ver+1)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    # 단어 네트워크 \n",
    "    wd_network = pd.DataFrame(index=range(len(corpus)),columns=[\"corpus\",\"mdl.docs\",\"Topic\",\"Topic_prob\"])\n",
    "\n",
    "    for u in tqdm(range(len(corpus))):\n",
    "        wd_network.iloc[u][0] =  ' '.join(map(str, corpus[u]))\n",
    "\n",
    "    for j in tqdm(range(len(mdl.docs))):\n",
    "        wd_network.iloc[j][1] = mdl.docs[j]\n",
    "\n",
    "    for z in tqdm(range(len(mdl.docs))):\n",
    "        wd_network.iloc[z][2] = mdl.docs[z].get_topics(top_n=mdl.k)[0][0] + 1\n",
    "\n",
    "    for c in tqdm(range(len(mdl.docs))):\n",
    "        wd_network.iloc[c][3] = mdl.docs[c].get_topics(top_n=mdl.k)[0][1]\n",
    "        \n",
    "    topic_len = int(topic_num)\n",
    "    \n",
    "    for topic_num in tqdm(range(int(topic_len))):\n",
    "        using_word= []\n",
    "        try:\n",
    "            for i in range(500):\n",
    "                using_word.append(mdl.get_topic_words(topic_num,top_n=500)[i][0]) \n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            if len(wd_network[wd_network[\"Topic\"]==topic_num+1].sort_values(\"Topic_prob\",ascending=False)) < 100:\n",
    "                a = len(wd_network[wd_network[\"Topic\"]==topic_num+1].sort_values(\"Topic_prob\",ascending=False))\n",
    "            else :\n",
    "                a = 100 \n",
    "            for i in range(a):\n",
    "                temp = wd_network[wd_network[\"Topic\"]==topic_num+1].sort_values(\"Topic_prob\",ascending=False).iloc[i][0]\n",
    "                temp2 = []\n",
    "                for j in (temp.split(\" \")):\n",
    "                    if j in using_word:\n",
    "                        temp2.append(j)\n",
    "                    else:\n",
    "                        continue\n",
    "                count = {}\n",
    "                for c,a in enumerate(temp2):  # i는 숫자 a는 1행 \n",
    "                    for b in temp2[c+1:]:\n",
    "                        if a>b:\n",
    "                            count[b,a] = count.get((b,a),0)+1\n",
    "                        else:\n",
    "                            count[a,b] = count.get((a,b),0)+1\n",
    "                word_df = pd.DataFrame.from_dict(count,orient=\"index\")  \n",
    "                df = pd.concat([df,word_df])\n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "            df[1] = pd.DataFrame(df[\"index\"].tolist())[0]\n",
    "            df[2] = pd.DataFrame(df[\"index\"].tolist())[1]\n",
    "            df = df[df[1]!=df[2]]\n",
    "            df= pd.DataFrame(df.groupby(\"index\")[0].sum())\n",
    "\n",
    "            list1 = []\n",
    "            for i in range(len(df)):\n",
    "                list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "            df2 = pd.DataFrame(list1,columns=[\"term1\",\"term2\",\"freq\"])\n",
    "            df3 = df2.sort_values(by=[\"freq\"],ascending=False)\n",
    "            df3 = df3.reset_index(drop=True)\n",
    "\n",
    "            i =1\n",
    "            while len((np.where(df3[\"freq\"]>=i))[0])>100:\n",
    "                i +=1\n",
    "            freq_num=i\n",
    "\n",
    "            G_centrality = nx.Graph()\n",
    "            for i in range(len((np.where(df3[\"freq\"]>=freq_num))[0])):\n",
    "                G_centrality.add_edge(df3[\"term1\"][i],df3[\"term2\"][i],weight=int(df3[\"freq\"][i]))\n",
    "\n",
    "            dgr = nx.degree_centrality(G_centrality)      #연결 중심성\n",
    "            btw = nx.betweenness_centrality(G_centrality) #매개 중심성\n",
    "            cls = nx.closeness_centrality(G_centrality)   #근접 중심성\n",
    "            egv = nx.eigenvector_centrality(G_centrality, tol=1.0e-3) #고유벡터 중심성\n",
    "            pgr = nx.pagerank(G_centrality) #페이지랭크 안됨 \n",
    "\n",
    "            sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            sorted_pgr = sorted(pgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "            G= nx.Graph()\n",
    "\n",
    "            for i in range(len(sorted_pgr)):\n",
    "                G.add_node(sorted_pgr[i][0],nodesize=sorted_dgr[i][1])\n",
    "            for i in range(len((np.where(df3[\"freq\"]>=freq_num))[0])):\n",
    "                G.add_weighted_edges_from([(df3[\"term1\"][i],df3[\"term2\"][i],int(df3[\"freq\"][i]))])\n",
    "\n",
    "            sizes = [G.nodes[node][\"nodesize\"]*2000 for node in G]\n",
    "\n",
    "            ## 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "            mpl.rcParams['axes.unicode_minus'] = False\n",
    "            font_fname = \"C:\\\\Windows\\\\Fonts\\\\NanumGothicCoding-bold.ttf\"\n",
    "            fontprop = fm.FontProperties(fname=font_fname,size=10).get_name()\n",
    "\n",
    "            options={\n",
    "                \"edge_color\":'#FFDEA2',\n",
    "                \"width\":1,\n",
    "                \"with_labels\":True,\n",
    "                \"font_weight\":\"bold\",\n",
    "            }\n",
    "\n",
    "            plt.figure(figsize=(16,8)); \n",
    "            nx.draw_networkx(G,node_size=sizes,pos=nx.kamada_kawai_layout(G),**options,font_family=fontprop)\n",
    "            ax = plt.gca()\n",
    "            ax.collections[0].set_edgecolor(\"#555555\")\n",
    "\n",
    "            plt.savefig(\"C:\\\\Users\\\\KISDI\\\\LDA\\\\html\\\\LDA\\\\NTIS\\\\Topic_Network_NTIS_middle\\\\\"+\"Topic_\"+str(topic_ver+1) +\"\\\\Topic_\" + str(topic_num+1)  +\".png\", bbox_inches='tight')\n",
    "        except:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5cb8f-2d4e-45c9-8761-78f968911cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6392df19",
   "metadata": {},
   "source": [
    "# 하나씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2309e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ver = int(input(\"불러올 토픽번호를 입력하세요 :\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa825a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"C:\\\\Users\\\\KISDI\\\\LDA\\\\data\\\\LDA\\\\NTIS\\\\corpus_topic\\\\\"\n",
    "# 코퍼스 다운로드\n",
    "corpus = []\n",
    "for n, line in enumerate(open(DATA_DIR + \"LDA_topic_\"+str(topic_ver)+\".txt\", encoding='CP949')):\n",
    "    doc=line.strip().split()\n",
    "    corpus.append(doc)\n",
    "# 모델 다운로드\n",
    "mdl=tp.LDAModel.load(\"./model/middle_model/LDA_MODEL_topic_\"+str(topic_ver)+\".bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_network = pd.DataFrame(index=range(len(corpus)),columns=[\"corpus\",\"mdl.docs\",\"Topic\",\"Topic_prob\"])\n",
    "\n",
    "for u in tqdm(range(len(corpus))):\n",
    "    wd_network.iloc[u][0] =  ' '.join(map(str, corpus[u]))\n",
    "\n",
    "for j in tqdm(range(len(mdl.docs))):\n",
    "    wd_network.iloc[j][1] = mdl.docs[j]\n",
    "\n",
    "for z in tqdm(range(len(mdl.docs))):\n",
    "    wd_network.iloc[z][2] = mdl.docs[z].get_topics(top_n=mdl.k)[0][0] + 1\n",
    "\n",
    "for c in tqdm(range(len(mdl.docs))):\n",
    "    wd_network.iloc[c][3] = mdl.docs[c].get_topics(top_n=mdl.k)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36178d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_num = int(input(\"MIDDLE 토픽번호를 입력하세요 :\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049db6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using_word= []\n",
    "\n",
    "for i in range(500):\n",
    "    using_word.append(mdl.get_topic_words(topic_num-1,top_n=500)[i][0]) \n",
    "\n",
    "df = pd.DataFrame()\n",
    "if len(wd_network[wd_network[\"Topic\"]==topic_num].sort_values(\"Topic_prob\",ascending=False)) < 100:\n",
    "    a = len(wd_network[wd_network[\"Topic\"]==topic_num].sort_values(\"Topic_prob\",ascending=False))\n",
    "else :\n",
    "    a = 100 \n",
    "for i in range(a):\n",
    "    temp = wd_network[wd_network[\"Topic\"]==topic_num].sort_values(\"Topic_prob\",ascending=False).iloc[i][0]\n",
    "    temp2 = []\n",
    "    for j in (temp.split(\" \")):\n",
    "        if j in using_word:\n",
    "            temp2.append(j)\n",
    "        else:\n",
    "            continue\n",
    "    count = {}\n",
    "    for c,a in enumerate(temp2):  # i는 숫자 a는 1행 \n",
    "        for b in temp2[c+1:]:\n",
    "            if a>b:\n",
    "                count[b,a] = count.get((b,a),0)+1\n",
    "            else:\n",
    "                count[a,b] = count.get((a,b),0)+1\n",
    "    word_df = pd.DataFrame.from_dict(count,orient=\"index\")  \n",
    "    df = pd.concat([df,word_df])\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df[1] = pd.DataFrame(df[\"index\"].tolist())[0]\n",
    "df[2] = pd.DataFrame(df[\"index\"].tolist())[1]\n",
    "df = df[df[1]!=df[2]]\n",
    "df= pd.DataFrame(df.groupby(\"index\")[0].sum())\n",
    "\n",
    "list1 = []\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "df2 = pd.DataFrame(list1,columns=[\"term1\",\"term2\",\"freq\"])\n",
    "df3 = df2.sort_values(by=[\"freq\"],ascending=False)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n",
    "i =1\n",
    "while len((np.where(df3[\"freq\"]>=i))[0])>100:\n",
    "    i +=1\n",
    "freq_num=i\n",
    "\n",
    "G_centrality = nx.Graph()\n",
    "for i in range(len((np.where(df3[\"freq\"]>=freq_num))[0])):\n",
    "    G_centrality.add_edge(df3[\"term1\"][i],df3[\"term2\"][i],weight=int(df3[\"freq\"][i]))\n",
    "\n",
    "dgr = nx.degree_centrality(G_centrality)      #연결 중심성\n",
    "btw = nx.betweenness_centrality(G_centrality) #매개 중심성\n",
    "cls = nx.closeness_centrality(G_centrality)   #근접 중심성\n",
    "egv = nx.eigenvector_centrality(G_centrality, tol=1.0e-3) #고유벡터 중심성\n",
    "pgr = nx.pagerank(G_centrality) #페이지랭크 안됨 \n",
    "\n",
    "sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_pgr = sorted(pgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "G= nx.Graph()\n",
    "\n",
    "for i in range(len(sorted_pgr)):\n",
    "    G.add_node(sorted_pgr[i][0],nodesize=sorted_dgr[i][1])\n",
    "for i in range(len((np.where(df3[\"freq\"]>=freq_num))[0])):\n",
    "    G.add_weighted_edges_from([(df3[\"term1\"][i],df3[\"term2\"][i],int(df3[\"freq\"][i]))])\n",
    "\n",
    "sizes = [G.nodes[node][\"nodesize\"]*2000 for node in G]\n",
    "\n",
    "## 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "font_fname = \"C:\\\\Windows\\\\Fonts\\\\NanumGothicCoding-bold.ttf\"\n",
    "fontprop = fm.FontProperties(fname=font_fname,size=10).get_name()\n",
    "\n",
    "options={\n",
    "    \"edge_color\":'#FFDEA2',\n",
    "    \"width\":1,\n",
    "    \"with_labels\":True,\n",
    "    \"font_weight\":\"bold\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(16,8)); \n",
    "nx.draw_networkx(G,node_size=sizes,pos=nx.spring_layout(G,k=3.5,iterations=100),**options,font_family=fontprop)\n",
    "ax = plt.gca()\n",
    "ax.collections[0].set_edgecolor(\"#555555\")\n",
    "\n",
    "plt.savefig(\"C:\\\\Users\\\\KISDI\\\\LDA\\\\html\\\\LDA\\\\NTIS\\\\Topic_Network_NTIS_middle\\\\\"+\"Topic_\"+str(topic_ver) +\"\\\\Topic_\" + str(topic_num)  +\".png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0f7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
