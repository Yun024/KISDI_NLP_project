{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "\n",
    "version_name=str(datetime.today().strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\newcomer02\\\\NTIS_Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토픽분포 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "토픽 개수를 입력하세요 :  20\n",
      "시작 년도를 입력하세요 :  2017\n"
     ]
    }
   ],
   "source": [
    "topic_num=int(input(\"토픽 개수를 입력하세요 : \"))\n",
    "start_year=int(input(\"시작 년도를 입력하세요 : \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "./data/prediction 내 사용할 데이터 폴더명을 입력하세요 (Default:엔터키):  REPORT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REPORT 데이터의 토픽 분포를 예측합니다.\n"
     ]
    }
   ],
   "source": [
    "folder_name = input(\"./data/prediction 내 사용할 데이터 폴더명을 입력하세요 (Default:엔터키): \")\n",
    "path='./data/Default/'+folder_name+'/prediction/'+folder_name+'_topic_dist_'+str(topic_num)+'.pkl'\n",
    "print(\"\\n%s 데이터의 토픽 분포를 예측합니다.\"%(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Real_Predict_Measure(path):\n",
    "    DATA = pd.read_pickle(path)\n",
    "\n",
    "    DATA = DATA.T\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "\n",
    "    for k in range(len(DATA)-4):\n",
    "        for i in range(len(DATA.columns)):\n",
    "            train_X.append(list(DATA.iloc[k:k+3,i]))\n",
    "            train_Y.append(DATA.iloc[k+3,i])\n",
    "\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    for i in range(len(DATA.columns)):\n",
    "        test_X.append(list(DATA.iloc[1:4,i]))\n",
    "    test_X = np.array(test_X)\n",
    "\n",
    "    for i in range(len(DATA.columns)):\n",
    "        test_Y.append(DATA.iloc[4,i])\n",
    "    test_Y = np.array(test_Y)\n",
    "    \n",
    "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1],1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, \n",
    "                   input_shape=(train_X.shape[1], train_X.shape[2]), \n",
    "                   activation='relu', \n",
    "                   return_sequences=False)\n",
    "              )\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(train_X, train_Y, epochs=50, batch_size=2)\n",
    "    \n",
    "    test_X = test_X.reshape((len(DATA.columns),3,1))\n",
    "    yhat = model.predict(test_X)\n",
    "    \n",
    "    Y_col = []\n",
    "    for i in yhat:\n",
    "        for j in i:\n",
    "            Y_col.append(j)\n",
    "    \n",
    "    Y_test = list(test_Y)\n",
    "    \n",
    "    mdl = model\n",
    "    mse = mean_squared_error(Y_test, Y_col)\n",
    "    r2 = r2_score(Y_test, Y_col)\n",
    "    kld = kl_divergence(Y_test, Y_col)\n",
    "    DATA = DATA.T\n",
    "    DATA[str(len(DATA.columns))] = Y_col\n",
    "    \n",
    "    return DATA, Y_col, Y_test, mse, r2, kld, mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random as rn\n",
    "# import tensorflow as tf\n",
    "# seed_num = 10\n",
    "# np.random.seed(seed_num)\n",
    "# rn.seed(seed_num)\n",
    "# tf.random.set_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA, Y_col, Y_test, mse, r2, kld, mdl = Real_Predict_Measure(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 0.000366\n",
      "\n",
      "R-Square : 0.904634\n",
      "\n",
      "KL-Divergence : -0.026746\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE : %f\"%(mse))\n",
    "print(\"\\nR-Square : %f\"%(r2))\n",
    "print(\"\\nKL-Divergence : %f\"%(kld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns= [0,1,2,3,4,\"predict_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "def All_Plot1(DATA, Y_col, Y_test):\n",
    "    top_list = []\n",
    "    for i in range(len(DATA)):\n",
    "        top_list.append('Topic '+str(i+1))\n",
    "\n",
    "    top_pred_list = []\n",
    "    for i in top_list:\n",
    "        top_pred_list.append(i+' Pred')\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(len(DATA)):\n",
    "        new_df['Topic '+str(i+1)] = DATA.iloc[i]\n",
    "\n",
    "    year_list = []\n",
    "    for i in range(len(DATA.columns)-1):\n",
    "        year_list.append(start_year+i)\n",
    "\n",
    "    yr_topic_norm = []\n",
    "    for i in range(len(DATA.columns)-1):\n",
    "        yr_topic_norm.append(list(DATA[i]))\n",
    "\n",
    "    topic_dist_norm = []\n",
    "    for j in range(len(DATA)):\n",
    "        topic_yr = []\n",
    "        for i in range(len(DATA.columns)-1):\n",
    "            topic_yr.append(yr_topic_norm[i][j])\n",
    "        topic_dist_norm.append(topic_yr)\n",
    "\n",
    "    top_id = []\n",
    "    for i in range(len(DATA)):\n",
    "        top_id.append(i+1)\n",
    "\n",
    "    index = np.arange(len(DATA))\n",
    "    bar_width = 0.35\n",
    "    plt.figure(figsize=(15,3))\n",
    "    p1 = plt.bar(index, Y_test, bar_width)\n",
    "    p2 = plt.bar(index+bar_width, Y_col, bar_width)\n",
    "    plt.title('LSTM '+str(start_year+len(DATA.columns)-2)+' Topic Distribution') \n",
    "    plt.xlabel('Topic', fontsize=10)\n",
    "    plt.ylabel('Probability', fontsize=10)\n",
    "    plt.xticks(index, top_id, fontsize=10)\n",
    "    plt.legend((p1[0], p2[0]), ('Prediction', 'Real'), fontsize=10)\n",
    "    #plt.savefig(DIR+'/All_Topics_Verification')\n",
    "    plt.show()\n",
    "\n",
    "    top_pred = []\n",
    "    for j in range(len(DATA)):\n",
    "        topic_yr = []\n",
    "        for i in range(len(DATA.columns)-2):\n",
    "            topic_yr.append(topic_dist_norm[j][i])\n",
    "        topic_yr.append(Y_col[j])\n",
    "        top_pred.append(topic_yr)\n",
    "        \n",
    "    for i in range(len(DATA)):\n",
    "        plt.title(folder_name+'_Topic ' + str(i+1) + ' Distribution')\n",
    "        plt.grid()\n",
    "        labels = top_list\n",
    "\n",
    "        plt.plot(year_list, topic_dist_norm[i], label = top_pred_list[i], linestyle = 'dotted')\n",
    "        p1 = plt.scatter(year_list, topic_dist_norm[i])\n",
    "        plt.plot(year_list, top_pred[i], label = top_list[i])\n",
    "        p2 = plt.scatter(year_list, top_pred[i])\n",
    "\n",
    "        plt.plot()\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xticks(np.arange(start_year,start_year+len(DATA.columns)-1),labels = [i for i in list(range(start_year,start_year+len(DATA.columns)-1))])\n",
    "        plt.tick_params(axis='x', direction='in', length=3, pad=6, labelsize=14, top=True)\n",
    "        #plt.savefig(DIR+'/'+version+'_topic_' + str(i+1)+'_verification')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "All_Plot1(DATA, Y_col, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_Predict_Measure(path):\n",
    "    DATA = pd.read_pickle(path)\n",
    "\n",
    "    DATA = DATA.T\n",
    "    \n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    \n",
    "    for k in range(len(DATA)-3):\n",
    "        for i in range(len(DATA.columns)):\n",
    "            train_X.append(list(DATA.iloc[k:k+3,i]))\n",
    "            train_Y.append(DATA.iloc[k+3,i])\n",
    "    train_X = np.array(train_X)\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    for i in range(len(DATA.columns)):\n",
    "        test_X.append(list(DATA.iloc[len(DATA)-3:len(DATA),i]))\n",
    "    test_X = np.array(test_X)\n",
    "    \n",
    "    print(train_X.shape)\n",
    "    print(train_Y.shape)\n",
    "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1],1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, \n",
    "                   input_shape=(train_X.shape[1], train_X.shape[2]), \n",
    "                   activation='relu', \n",
    "                   return_sequences=False)\n",
    "              )\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(train_X, train_Y, epochs=50, batch_size=5)\n",
    "    \n",
    "    test_X= test_X.reshape((len(DATA.columns),3,1))\n",
    "    yhat = model.predict(test_X)\n",
    "    \n",
    "    Y_col = []\n",
    "    for i in yhat:\n",
    "        for j in i:\n",
    "            Y_col.append(j)\n",
    "    \n",
    "    DATA = DATA.T\n",
    "#    DATA[len(DATA.columns)] = Y_col\n",
    "    mdl = model\n",
    "    \n",
    "    return DATA, mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA, mdl = New_Predict_Measure(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predict_Data(DATA,step):\n",
    "    temp_data = DATA\n",
    "    temp_col = len(DATA.columns)\n",
    "    for j in range(step) :\n",
    "        temp_x =[]\n",
    "        for i in range(len(DATA)) :\n",
    "            temp_x.append(list(temp_data.iloc[i,-3:]))\n",
    "        temp_x = np.array(temp_x).reshape((topic_num,3,1))\n",
    "        temp_y = mdl.predict(np.array(temp_x))\n",
    "        temp_data[temp_col+j] = temp_y\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = Make_Predict_Data(DATA,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.columns= [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "def All_Plot2(DATA,step):\n",
    "    DIR = \"C:\\\\Users\\\\KISDI\\\\LDA\\\\html\\\\LSTM\\\\\"+folder_name+\"\\\\\"\n",
    "    top_list = []\n",
    "    for i in range(len(DATA)):\n",
    "        top_list.append('Topic '+str(i+1))\n",
    "\n",
    "    top_pred_list = []\n",
    "    for i in top_list:\n",
    "        top_pred_list.append(i+' Pred')\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(len(DATA)):\n",
    "        new_df['Topic '+str(i+1)] = DATA.iloc[i]\n",
    "    \n",
    "    year_list = []\n",
    "    for i in range(len(DATA.columns)):\n",
    "        year_list.append(start_year+i)\n",
    "\n",
    "    yr_topic_norm = []\n",
    "    for i in range(len(DATA.columns)):\n",
    "        yr_topic_norm.append(list(DATA[i]))\n",
    "\n",
    "    topic_dist_norm = []\n",
    "    for j in range(len(DATA)):\n",
    "        topic_yr = []\n",
    "        for i in range(len(DATA.columns)-1):\n",
    "            topic_yr.append(yr_topic_norm[i][j])\n",
    "\n",
    "        topic_dist_norm.append(topic_yr)\n",
    "\n",
    "    topic_sum = []\n",
    "    for i in topic_dist_norm:\n",
    "        c = 0\n",
    "        c+=sum(i)\n",
    "        topic_sum.append(c)\n",
    "\n",
    "    topic_sort = sorted(topic_sum, reverse=True)    \n",
    "\n",
    "    idx_sort = []\n",
    "    for i in topic_sort:\n",
    "        idx_sort.append(topic_sum.index(i))\n",
    "\n",
    "    top_id = []\n",
    "    for i in range(len(DATA)):\n",
    "        top_id.append(i+1)\n",
    "\n",
    "    top_pred = []\n",
    "    for j in range(len(DATA)):\n",
    "        topic_yr = []\n",
    "        for i in range(len(DATA.columns)-1):\n",
    "            topic_yr.append(topic_dist_norm[j][i])\n",
    "        topic_yr.append(Y_col[j])\n",
    "        top_pred.append(topic_yr)\n",
    "    \n",
    "    for i in range(len(DATA)):\n",
    "        plt.title(folder_name+'_Topic ' + str(i+1) + ' Prediction')\n",
    "        plt.grid()\n",
    "        labels = top_list\n",
    "\n",
    "        plt.plot(year_list, top_pred[i], label = top_pred_list[i], linestyle = 'dotted')\n",
    "        p2 = plt.scatter(year_list, top_pred[i])\n",
    "        plt.plot(year_list[:-step], topic_dist_norm[i][:-step+1], label = top_list[i])\n",
    "        p1 = plt.scatter(year_list[:-step], topic_dist_norm[i][:-step+1])\n",
    "\n",
    "        plt.plot()\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xticks(np.arange(start_year,start_year+len(DATA.columns)),labels = [i for i in list(range(start_year,start_year+len(DATA.columns)))])\n",
    "        #plt.savefig(DIR + 'topic_' + str(i+1)+'_lstm')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "All_Plot2(pred_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABEL = []\n",
    "for i in range(len(DATA)):\n",
    "    DATA_LABEL.append('topic'+str(i+1))\n",
    "    \n",
    "DATA['LABEL']=DATA_LABEL\n",
    "SORTED_DATA=DATA.sort_values(by=[len(DATA.columns)-2],axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output type : ICT NTIS/NEWS 연도별 html 파일 (html/DTM 폴더)\n",
    "\n",
    "def ntis_time(DATA,top_n):\n",
    "    color=['brown','red','darkviolet','deeppink','forestgreen', 'fuchsia','indigo','lawngreen', 'lightslategray','yellow','silver','skyblue','tomato'', turquoise','yellowgreen']\n",
    "    plot=[]\n",
    "    \n",
    "    for i in range(top_n):\n",
    "        plot.append(go.Scatter(x = np.array(range(2014,2014+len(DATA.columns)-1)), y = SORTED_DATA.iloc[i,:-2], line=dict(color=color[i],width=4),mode = 'lines+markers', name = SORTED_DATA.iloc[i,-1]))\n",
    "        plot.append(go.Scatter(x = np.array(range(2014,2014+len(DATA.columns))), y = SORTED_DATA.iloc[i,:-1], line=dict(color=color[i],dash='dashdot',width=4),mode = 'lines+markers', name = 'predicted_'+SORTED_DATA.iloc[i,-1]))\n",
    "        \n",
    "    layout = go.Layout(title='NTIS 토픽별 트렌드(LSTM)',\n",
    "                       legend=dict(x=0,y=-1.7),margin=dict(l=20, r=20, t=60, b=300),paper_bgcolor=\"White\",\n",
    "                       autosize=True,title_font_size=30,font=dict(size=15),hoverlabel=dict(\n",
    "                                   \n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    ),\n",
    "                    xaxis=dict({\"tickvals\":list(range(2014,2014+len(DATA.columns))),\n",
    "                            \"ticktext\":[str(i) for i in list(range(2014,2014+len(DATA.columns)))],\n",
    "                           \"title\":\"연도\"}),\n",
    "                    yaxis=dict({\"title\":\"토픽 비중\"}),\n",
    "                    height=1000)\n",
    "    \n",
    "    gen_ntis = go.Figure(data=plot, layout=layout)\n",
    "    pyo.iplot(gen_ntis)\n",
    "    \n",
    "    gen_ntis.write_html(DIR+'/'+\"ntis_trace_상위\"+str(top_n)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntis_time(DATA,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어분포 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "예측에 사용할 상위 단어 갯수를 지정하세요 (권장값=1000) :  1000\n"
     ]
    }
   ],
   "source": [
    "if version=='':\n",
    "    data=pd.read_pickle('./data/Default/NTIS/LDA/VAR,LSTM/WORDS_'+str(doc_type)+\"_\"+str(topic_num)+'.pkl')\n",
    "else:\n",
    "    data=pd.read_pickle('./data/PREDICTION/'+version+'/WORDS_'+str(topic_num)+'_t.pkl')\n",
    "    \n",
    "TOPN=int(input(\"예측에 사용할 상위 단어 갯수를 지정하세요 (권장값=1000) : \"))\n",
    "data=data[data.index<TOPN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마지막년도와 1년후 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lstm_mode(data):\n",
    "    \n",
    "    for y in range(topic_num):\n",
    "        ex_da=data[data['topic']==y+1]\n",
    "        train_X=ex_da.iloc[:,3:-2].values\n",
    "        train_Y=ex_da.iloc[:,-2:-1]\n",
    "        test_X=ex_da.iloc[:,4:-1].values\n",
    "        test_Y=ex_da.iloc[:,-1:]\n",
    "\n",
    "        train_X=train_X.reshape(train_X.shape[0], train_X.shape[1],1)\n",
    "        test_X=test_X.reshape(test_X.shape[0], test_X.shape[1],1)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(10, \n",
    "                       input_shape=(train_X.shape[1], train_X.shape[2]), \n",
    "                       activation='relu', \n",
    "                       return_sequences=False)\n",
    "                  )\n",
    "        model.add(Dense(5))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        history = model.fit(train_X, train_Y, epochs=30, batch_size=5)\n",
    "        yhat = model.predict(test_X)\n",
    "        \n",
    "        name1='predict_lstm'+str(data.columns[-1])\n",
    "        ex_da[name1]=yhat\n",
    "        norm=[]\n",
    "        for i in ex_da[name1]:\n",
    "            if i <=0:\n",
    "                norm.append(0)\n",
    "            else:\n",
    "                norm.append(i)\n",
    "        ex_da[name1]=norm\n",
    "        \n",
    "        ###########################################\n",
    "        \n",
    "        train_X=ex_da.iloc[:,3:-2].values\n",
    "        train_Y=ex_da.iloc[:,-2:-1]\n",
    "        test_X=ex_da.iloc[:,4:-1].values\n",
    "        test_Y=ex_da.iloc[:,-1:]\n",
    "\n",
    "        train_X=train_X.reshape(train_X.shape[0], train_X.shape[1],1)\n",
    "        test_X=test_X.reshape(test_X.shape[0], test_X.shape[1],1)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(10, \n",
    "                       input_shape=(train_X.shape[1], train_X.shape[2]), \n",
    "                       activation='relu', \n",
    "                       return_sequences=False)\n",
    "                  )\n",
    "        model.add(Dense(5))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        history = model.fit(train_X[1], train_Y[1], epochs=30, batch_size=5)\n",
    "        yhat = model.predict(test_X)\n",
    "        \n",
    "        name2='predict_lstm'+str(int(data.columns[-1])+1)\n",
    "        \n",
    "        ex_da[name2]=yhat\n",
    "        norm=[]\n",
    "        for i in ex_da[name2]:\n",
    "            if i <=0:\n",
    "                norm.append(0)\n",
    "            else:\n",
    "                norm.append(i)\n",
    "        ex_da[name2]=norm\n",
    "        \n",
    "        #############################################\n",
    "\n",
    "        if y==0:\n",
    "            final=ex_da\n",
    "        else:\n",
    "            final=pd.concat([final, ex_da])\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_data=lstm_mode(data)\n",
    "#final_data.to_csv(\"final_data_1.csv\",index=False)\n",
    "final_data = pd.read_csv(\"final_data_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step을 이용한 미래예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "몇년을 예측하고 싶은가요: 2\n"
     ]
    }
   ],
   "source": [
    "step = int(input(\"몇년을 예측하고 싶은가요:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_step_mode(data,step,train):\n",
    "    \n",
    "    for t in range(topic_num):\n",
    "\n",
    "        df_X = []\n",
    "        df_Y = []\n",
    "        ex_da=data[data['topic']==(t+1)]\n",
    "        for i in range(train):\n",
    "            df_X.append(ex_da.iloc[:,(3+i):(6+i)])\n",
    "            df_Y.append(ex_da.iloc[:,6+i:7+i])\n",
    "\n",
    "        for i in range(train):\n",
    "            df_X[i] = df_X[i].values.reshape(df_X[i].shape[0],df_X[i].shape[1],1)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(10,\n",
    "                      input_shape=(df_X[0].shape[1],df_X[0].shape[2]),\n",
    "                      activation=\"relu\",\n",
    "                      return_sequences=False)\n",
    "                 )\n",
    "        model.add(Dense(5))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "        for i in range(train):\n",
    "            model.fit(df_X[i],df_Y[i],epochs=30,batch_size=5)\n",
    "\n",
    "\n",
    "        temp_col = ex_da.columns.tolist()\n",
    "\n",
    "        for i in range(step):\n",
    "            test_X = ex_da.iloc[:,-3:].values.reshape(df_X[0].shape[0],df_X[0].shape[1],1)\n",
    "            yhat = model.predict(test_X)\n",
    "            ex_da[temp_col[-1]+(i+1)] = yhat\n",
    "\n",
    "        if t==0:\n",
    "            final = ex_da\n",
    "        else:\n",
    "            final = pd.concat([final,ex_da])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_data = lstm_step_mode(data,step,len(data.columns)-6)\n",
    "### 2017~2021 기준 6은 2회학습 , 7은 1회학습\n",
    "### 년도가 늘어나면 학습횟수가 1회가 증가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_data.to_csv(\"final_data_2.csv\",index=False)\n",
    "final_data = pd.read_csv(\"final_data_2.csv\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018년도와 predict_18년도의 값을 비교하여 해당 데이터에 VAR성능이 어느 정도로 잘 나오는지 확인\n",
    "# 만약, Spearsman_corr 결과, 낮은 값들이 많다면 향후 년도 예측의 성능이 낮을 수 있음\n",
    "\n",
    "def spearsman_corr(fi_da):\n",
    "    topic_corr=[]\n",
    "    to_data=[]\n",
    "    year=fi_da.columns[-2]\n",
    "    predict_year=fi_da.columns[-1]\n",
    "    \n",
    "    for i in tqdm(range(1,len(fi_da['topic'].unique())+1)):\n",
    "        real=fi_da[fi_da['topic']==i][['word',year]]\n",
    "        real=real.sort_values(by=[year],ascending=False)\n",
    "        real['Number']=list(range(len(real)))\n",
    "        predict=fi_da[fi_da['topic']==i][['word',predict_year]]\n",
    "        predict=predict.sort_values(by=[predict_year],ascending=False)\n",
    "        predict['Number']=list(range(len(predict)))\n",
    "        predict.columns=['predict_word',predict_year,'predict_Number']\n",
    "        predict\n",
    "        for su in range(len(real)):\n",
    "            wo=list(real['word'])\n",
    "            data=predict[predict['predict_word']==wo[su]]\n",
    "            if su==0:\n",
    "                fo=data\n",
    "            else:\n",
    "                fo=pd.concat([fo,data])\n",
    "        fina=pd.concat([real,fo],axis=1)\n",
    "        to_data.append(fina)\n",
    "        topic_corr.append(fina[['Number','predict_Number']].corr(method='spearman').iloc[0,1])\n",
    "        \n",
    "    plt.figure(figsize=(25, 10))\n",
    "    to=pd.DataFrame(columns=['corr'])\n",
    "    \n",
    "    to['corr']=topic_corr\n",
    "    to.index=list(range(1,len(fi_da['topic'].unique())+1))\n",
    "    to['corr'].sort_values(ascending=False).plot(kind='bar')\n",
    "    \n",
    "    plt.title('TOPIC')\n",
    "    plt.ylabel('spearsman_corr')\n",
    "    #plt.savefig(DIR+'/'+data_type+'_topic_' + str(i+1)+'_spearman correlaion')\n",
    "    return to_data,to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 랭킹 변동 확인\n",
    "# 스피어만 상관계수의 범위는 -1 ~ 1 까지로, 비교적 높은 결과가 나옴 -> VAR로 향후 년도를 예측하는게 의미가 있다고 판단 \n",
    "\n",
    "da,to=spearsman_corr(final_data.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newcomer02\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\fromnumeric.py:3430: FutureWarning:\n",
      "\n",
      "In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "corr    0.676994\n",
       "dtype: float64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 단어 미래 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_plot2(data,topic,top_N,step):  \n",
    "\n",
    "    DIR = \"C:\\\\Users\\\\newcomer02\\\\NTIS_Project\\\\data\\\\Default\\\\NTIS\\\\LDA\\\\VAR,LSTM\\\\html\\\\\"+doc_type + \"\\\\topic_word_prediction_html\\\\\"\n",
    "    data = data[data[\"topic\"] == topic]\n",
    "    data = data.set_index(\"word\")\n",
    "    real_data = data.iloc[:,2:-(step)].T\n",
    "    predict_data = data.iloc[:,-(step)-1:].T\n",
    "    ye = real_data.index.append(predict_data.index).tolist()\n",
    "\n",
    "    col= real_data.columns.tolist()\n",
    "    color=['brown','red','darkviolet','deeppink','forestgreen', 'fuchsia','indigo','lawngreen', 'lightslategray','yellow','silver','skyblue','tomato'', turquoise','yellowgreen']\n",
    "    plot=[]\n",
    "    \n",
    "    for N in range(top_N):\n",
    "        #기존의 데이터\n",
    "        plot.append(go.Scatter(y=list(real_data[col[N]]), x=real_data.index.tolist(),name=col[N],line=dict(color=color[N], width=2),marker = dict(color=color[N]),mode='lines+markers'))\n",
    "        #예측데이터\n",
    "        plot.append(go.Scatter(y=list(predict_data[col[N]]), x=predict_data.index.tolist(),name=\"예측\"+str(col[N]),line=dict(color=color[N],dash=\"dashdot\", width=2),marker = dict(color=color[N]),mode='lines+markers'))\n",
    "    \n",
    "    layout = go.Layout(title=data[\"labels\"][0],autosize=True,\n",
    "                       xaxis=dict(\n",
    "                            tickvals=ye,\n",
    "                           title=\"연도\"),\n",
    "                       yaxis=dict(title = \"Topic내 단어 순위\",linewidth=2))\n",
    "\n",
    "    fig=go.Figure(data=plot,layout=layout)\n",
    "    \n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "            \"data\": plot,\n",
    "            \"layout\": go.Layout(autosize=True,height=500,width=700,title=data[\"labels\"][0],legend=dict(font=dict(size=15)),margin=dict(l=20, r=20,t=100),xaxis = dict(title = \"연도\",linewidth=0.5,\n",
    "                                tickvals=ye\n",
    "),yaxis=dict(title = \"Topic내 단어 순위\",linewidth=2))})#,auto_open=True,filename= DIR +\"topic_\" + str(topic) +\"_lstm.html\" ,image=\"png\",image_filename = \"topic_\" + str(topic) +\"_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(topic_num):\n",
    "    topic_word_plot2(data=final_data,topic=i+1,top_N=5,step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[final_data[\"topic\"]==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_di(data,word,number_of_topics):\n",
    "   \n",
    "    length=len(data['topic'].unique())\n",
    "    ds=[]\n",
    "    fib=[]\n",
    "    ra=[]\n",
    "    \n",
    "    name=list(data.labels.unique())\n",
    "    for rank in tqdm(range(1,length+1)):\n",
    "        word_da=data[data['topic']==rank].iloc[0:,3:]     \n",
    "        \n",
    "        ye=list(word_da.columns[0:-2])\n",
    "        ye.append(str(int(word_da.columns[-3])+1))\n",
    "        \n",
    "        for i in range(len(word_da.columns)):\n",
    "            word_da.iloc[:,i:i+1]=word_da.iloc[:,i:i+1].rank(ascending=False) \n",
    "        word_da['word']=data[data['topic']==rank].iloc[0:,0:1]     \n",
    "\n",
    "        ran=list(word_da.iloc[:,-3:-2].sum(axis=1))\n",
    "        word_da['rank']=ran\n",
    "        word_da['rank']=word_da['rank'].rank(ascending=True) \n",
    "        word_da.sort_values(by=['rank'],axis=0,inplace=True)\n",
    "        word_da=word_da.reset_index().iloc[:,1:-1]\n",
    "        word_das=word_da.set_index('word').T\n",
    "        \n",
    "        ds.append(word_das)\n",
    "   \n",
    "    for nu,dd in enumerate(range(len(ds))):\n",
    "        if word in ds[dd].columns:\n",
    "            ra.append(nu)\n",
    "            fib.append(ds[dd])\n",
    "            \n",
    "    if len(fib)!=0:\n",
    "   \n",
    "        for su,dat in enumerate(fib):\n",
    "            if su==0:\n",
    "                dat=pd.DataFrame(dat[word])\n",
    "                dat.columns=[word+'--'+str(name[su-1])]\n",
    "                finalss=dat\n",
    "            else:\n",
    "                dat=pd.DataFrame(dat[word])\n",
    "                dat.columns=[word+'--'+str(name[su-1])]\n",
    "                finalss=pd.concat([finalss,dat],axis=1)\n",
    "\n",
    "\n",
    "        finalss=finalss.T.sort_values(by=data.columns[-5]).T\n",
    "        \n",
    "        print('찾을수있는 number_of_topics의 최대길이 :',len(finalss.columns))\n",
    "\n",
    "        color=['fuchsia','brown','darkgray','darkviolet','deeppink','forestgreen','indigo','lawngreen', 'lightslategray','silver','skyblue','tomato'', turquoise','yellow','yellowgreen']\n",
    "\n",
    "        plot=[]\n",
    "\n",
    "        for N in range(number_of_topics):\n",
    "            yy=[]\n",
    "            i=0\n",
    "            for ys in range(len(ye)-1):\n",
    "                yy.append(i)\n",
    "                i+=1    \n",
    "\n",
    "            plot.append(go.Scatter(y=list(finalss.iloc[0:len(yy),N]), x=ye[0:-1],name=str(list(finalss.columns)[N]),line=dict(color=color[N], width=4),marker = dict(color=color[N]),mode='lines+markers'))\n",
    "\n",
    "            fin=finalss.drop([data.columns[-3]])\n",
    "\n",
    "            plot.append(go.Scatter(y=list(fin.iloc[:,N]), x=ye,name='예측_'+word,line=dict(color=color[N],dash='dashdot',width=4),marker = dict(color=color[N]),mode ='lines+markers'))\n",
    "\n",
    "        plotly.offline.iplot({\n",
    "            \"data\": plot,\n",
    "            \"layout\": go.Layout(title=word, xaxis = dict(title = \"년도\",linewidth=0.5),\n",
    "\n",
    "        yaxis=dict(autorange='reversed',title = \"Topic별 단어 순위\",linewidth=2),legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5,\n",
    "        xanchor=\"left\",\n",
    "        x=0\n",
    "    ))})\n",
    "\n",
    "        plotly.offline.init_notebook_mode(connected=False)\n",
    "\n",
    "        plotly.offline.plot({\n",
    "            \"data\": plot,\n",
    "            \"layout\": go.Layout(title=word,xaxis = dict(title = \"년도\",linewidth=0.5),\n",
    "\n",
    "        yaxis=dict(autorange='reversed',title = \"Topic내 해당 단어 순위\",linewidth=2),legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5,\n",
    "        xanchor=\"left\",\n",
    "        x=0\n",
    "    ))}, auto_open=False )#,filename=DIR+'/'+word)\n",
    "\n",
    "    else:\n",
    "        print('해당 단어는 데이터에 존재하지 않습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word=input(\"분석하고자 하는 단어를 입력하세요 : \")\n",
    "max_length = 5\n",
    "try:\n",
    "    topic_di(data=final_data,word=target_word,number_of_topics=max_length)\n",
    "except IndexError:\n",
    "    max_length -=1\n",
    "    topic_di(data=final_data,word=target_word,number_of_topics=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "mecab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
